{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv(r'C:\\Users\\vijay\\Desktop\\Machine Learning Course Batches\\FDP_ML_6t')\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "df.nunique()\n",
    "df.isnull().sum()\n",
    "df.duplicated().sum()\n",
    "df['total_bedrooms'].median()\n",
    "df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)\n",
    "for i in df.iloc[:,2:7]:\n",
    "    df[i] = df[i].astype('int')\n",
    "df.head()\n",
    "df.describe().T\n",
    "Numerical = df.select_dtypes(include=[np.number]).columns\n",
    "print(Numerical)\n",
    "for col in Numerical:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df[col].plot(kind='hist', title=col, bins=60, edgecolor='black')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "for col in Numerical:\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.boxplot(df[col], color='blue')\n",
    "    plt.title(col)\n",
    "    plt.ylabel(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e58e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['Target'] = data.target  # Adding the target variable (median house value)\n",
    "variable_meaning = {\n",
    "    \"MedInc\": \"Median income in block group\",\n",
    "    \"HouseAge\": \"Median house age in block group\",\n",
    "    \"AveRooms\": \"Average number of rooms per household\",\n",
    "    \"AveBedrms\": \"Average number of bedrooms per household\",\n",
    "    \"Population\": \"Population of block group\",\n",
    "    \"AveOccup\": \"Average number of household members\",\n",
    "    \"Latitude\": \"Latitude of block group\",\n",
    "    \"Longitude\": \"Longitude of block group\",\n",
    "    \"Target\": \"Median house value (in $100,000s)\"\n",
    "}\n",
    "variable_df = pd.DataFrame(list(variable_meaning.items()), columns=[\"Feature\", \"Description\"])\n",
    "print(\"\\nVariable Meaning Table:\")\n",
    "print(variable_df)\n",
    "print(\"\\nBasic Information about Dataset:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst Five Rows of Dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "summary_explanation = \"\"\"\n",
    "The summary statistics table provides key percentiles and other descriptive metrics.\n",
    "\n",
    "**25% (First Quartile - Q1):** This represents the value below which 25% of the data falls.\n",
    "**50% (Median - Q2):** This is the middle value when the data is sorted. It provides the central tendency.\n",
    "**75% (Third Quartile - Q3):** This represents the value below which 75% of the data falls.\n",
    "\n",
    "These percentiles are useful for detecting skewness, data distribution, and identifying potential outliers \n",
    "(values beyond Q1 - 1.5*IQR or Q3 + 1.5*IQR).\n",
    "\"\"\"\n",
    "print(\"\\nSummary Statistics Explanation:\")\n",
    "print(summary_explanation)\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(df.isnull().sum())\n",
    "plt.figure(figsize=(12, 8))\n",
    "df.hist(figsize=(12, 8), bins=30, edgecolor='black')\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplots of Features to Identify Outliers\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "sns.pairplot(df[['MedInc', 'HouseAge', 'AveRooms', 'Target']], diag_kind='kde')\n",
    "plt.show()\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"1. The dataset has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "print(\"2. No missing values were found in the dataset.\")\n",
    "print(\"3. Histograms show skewed distributions in some features like 'MedInc'.\")\n",
    "print(\"4. Boxplots indicate potential outliers in 'AveRooms' and 'AveOccup'.\")\n",
    "print(\"5. Correlation heatmap shows 'MedInc' has the highest correlation with house prices.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f00571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = iris.target_names\n",
    "for i in range(len(colors)):\n",
    "    ax.scatter(X_scaled[y == i, 0], X_scaled[y == i, 1], X_scaled[y == i, 2], color=colors[i], label=labels[i])\n",
    "ax.set_xlabel('Sepal Length')\n",
    "ax.set_ylabel('Sepal Width')\n",
    "ax.set_zlabel('Petal Length')\n",
    "ax.set_title('3D Visualization of Iris Data Before PCA')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "U, S, Vt = np.linalg.svd(X_scaled, full_matrices=False)\n",
    "print(\"Singular Values:\", S)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance by PC1: {explained_variance[0]:.2f}\")\n",
    "print(f\"Explained Variance by PC2: {explained_variance[1]:.2f}\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(colors)):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=colors[i], label=labels[i])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA on Iris Dataset (Dimensionality Reduction)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(len(colors)):\n",
    "    ax.scatter(X_scaled[y == i, 0], X_scaled[y == i, 1], X_scaled[y == i, 2], color=colors[i], label=labels[i])\n",
    "for i in range(3):  # Plot first three eigenvectors\n",
    "    ax.quiver(0, 0, 0, eigenvectors[i, 0], eigenvectors[i, 1], eigenvectors[i, 2], color='black', length=1)\n",
    "ax.set_xlabel('Sepal Length')\n",
    "ax.set_ylabel('Sepal Width')\n",
    "ax.set_zlabel('Petal Length')\n",
    "ax.set_title('3D Data with Eigenvectors')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"salgodataset.csv\")  # Replace with your CSV file path\n",
    "print(data)\n",
    "print(\"Training Data:\")\n",
    "def find_s_algorithm(data):\n",
    "    \"\"\"Implements the Find-S algorithm to find the most specific hypothesis.\"\"\"\n",
    "    attributes = data.iloc[:, :-1].values  # All columns except last\n",
    "    target = data.iloc[:, -1].values       # Last column (class labels)\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == \"Yes\":  # Consider only positive examples\n",
    "            hypothesis = attributes[i].copy()\n",
    "            break\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == \"Yes\":\n",
    "            for j in range(len(hypothesis)):\n",
    "                if hypothesis[j] != attributes[i][j]:\n",
    "                    hypothesis[j] = '?'  # Generalize inconsistent attributes\n",
    "    return hypothesis\n",
    "final_hypothesis = find_s_algorithm(data)\n",
    "print(\"Most Specific Hypothesis:\", final_hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "x_values = np.random.rand(100)\n",
    "labels = np.array([\"Class1\" if x <= 0.5 else \"Class2\" for x in x_values[:50]])\n",
    "print(x_values)\n",
    "print(\"--------------------------------------\")\n",
    "print(labels)\n",
    "def knn_classify(x_train, y_train, x_test, k):\n",
    "    predictions = []\n",
    "    for x in x_test:\n",
    "        distances = np.abs(x_train - x)  # Absolute distance\n",
    "        k_nearest_indices = np.argsort(distances)[:k]  # Indices of k nearest neighbors\n",
    "        k_nearest_labels = y_train[k_nearest_indices]  # Labels of nearest neighbors\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)[0][0]  # Most frequent label\n",
    "        predictions.append(most_common)\n",
    "    return np.array(predictions)\n",
    "k_values = [1, 2, 3, 4, 5, 20, 30]\n",
    "results = {}\n",
    "for k in k_values:\n",
    "    predicted_labels = knn_classify(x_values[:50], labels, x_values[50:], k)\n",
    "    results[k] = predicted_labels\n",
    "for k in k_values:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_values[:50], [1]*50, c=[\"blue\" if lbl == \"Class1\" else \"red\" for lbl in labels], label=\"Labeled Data\")\n",
    "    plt.scatter(x_values[50:], [2]*50, c=[\"blue\" if lbl == \"Class1\" else \"red\" for lbl in results[k]], label=f\"Classified Data (k={k})\")\n",
    "    plt.xlabel(\"x values\")\n",
    "    plt.ylabel(\"Classified/Unclassified\")\n",
    "    plt.title(f\"KNN Classification Clusters (k={k})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "for k, preds in results.items():\n",
    "    print(f\"Results for k={k}:\")\n",
    "    print(preds)\n",
    "    print(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd4f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def gaussian_kernel(x, xi, tau):\n",
    "    return np.exp(-np.sum((x - xi) ** 2) / (2 * tau ** 2))\n",
    "def locally_weighted_regression(x, X, y, tau):\n",
    "    m = X.shape[0]\n",
    "    weights = np.array([gaussian_kernel(x, X[i], tau) for i in range(m)])\n",
    "    W = np.diag(weights)\n",
    "    X_transpose_W = X.T @ W\n",
    "    theta = np.linalg.inv(X_transpose_W @ X) @ X_transpose_W @ y\n",
    "    return x @ theta\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 2 * np.pi, 100)\n",
    "y = np.sin(X) + 0.1 * np.random.randn(100)\n",
    "X_bias = np.c_[np.ones(X.shape), X]\n",
    "x_test = np.linspace(0, 2 * np.pi, 200)\n",
    "x_test_bias = np.c_[np.ones(x_test.shape), x_test]\n",
    "tau = 0.5\n",
    "y_pred = np.array([locally_weighted_regression(xi, X_bias, y, tau) for xi in x_test_bias])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='red', label='Training Data', alpha=0.7)\n",
    "plt.plot(x_test, y_pred, color='blue', label=f'LWR Fit (tau={tau})', linewidth=2)\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Locally Weighted Regression', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = sns.load_dataset(\"mpg\").dropna(subset=['horsepower', 'mpg'])\n",
    "X = df[['horsepower']]\n",
    "y = df['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "model = LinearRegression().fit(X_train_poly, y_train)\n",
    "X_range = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "X_range_poly = poly.transform(X_range)\n",
    "y_pred_range = model.predict(X_range_poly)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, color='blue', alpha=0.6, label='Data')\n",
    "plt.plot(X_range, y_pred_range, color='red', label='Polynomial Fit')\n",
    "plt.xlabel('Horsepower')\n",
    "plt.ylabel('MPG')\n",
    "plt.title('Polynomial Regression (Degree 2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "data = pd.read_csv(r\"C:\\Users\\PAVITHRA H R\\Downloads\\Boston housing dataset.csv\")\n",
    "if data.isnull().values.any():\n",
    "    print(\"Filling missing values with column means.\")\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "X = data.drop(columns='MEDV')\n",
    "y = data['MEDV']\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.2f}\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.6, edgecolors=\"k\", s=80)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.xlabel('Actual MEDV')\n",
    "plt.ylabel('Predicted MEDV')\n",
    "plt.title('Actual vs Predicted House Prices')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"\\n--- Decision Tree ---\")\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, filled=True)\n",
    "plt.title('Decision Tree - Breast Cancer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "data = fetch_olivetti_faces()\n",
    "data.keys()\n",
    "def print_faces(images, target, top_n):\n",
    "    top_n = min(top_n, len(images))\n",
    "    grid_size = int(np.ceil(np.sqrt(top_n)))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "    for i, ax in enumerate(axes.ravel()):\n",
    "        if i < top_n:\n",
    "            ax.imshow(images[i], cmap='bone')\n",
    "            ax.axis('off')\n",
    "            ax.text(2, 12, str(target[i]), fontsize=9, color='red')\n",
    "            ax.text(2, 55, f\"face: {i}\", fontsize=9, color='blue')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "plt.show()\n",
    "print_faces(data.images,data.target,10)\n",
    "print(\"\\n--- Naive Bayes - Olivetti Faces ---\")\n",
    "faces = fetch_olivetti_faces()\n",
    "X, y = faces.data, faces.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "print(\"\\n--- K-Means Clustering ---\")\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "model = KMeans(n_clusters=2, n_init=10)\n",
    "model.fit(X)\n",
    "labels = model.labels_\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.title('K-Means Clustering - Breast Cancer')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
