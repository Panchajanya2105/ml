{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv(r'/home/srujan-panchajanya-s-s/ML/housing.csv')\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "df.nunique()\n",
    "df.isnull().sum()\n",
    "df.duplicated().sum()\n",
    "df['total_bedrooms'].median()\n",
    "df['total_bedrooms'].fillna(df['total_bedrooms'].median(), inplace=True)\n",
    "for i in df.iloc[:,2:7]:\n",
    "    df[i] = df[i].astype('int')\n",
    "df.head()\n",
    "df.describe().T\n",
    "Numerical = df.select_dtypes(include=[np.number]).columns\n",
    "print(Numerical)\n",
    "for col in Numerical:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df[col].plot(kind='hist', title=col, bins=60, edgecolor='black')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "for col in Numerical:\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.boxplot(df[col], color='blue')\n",
    "    plt.title(col)\n",
    "    plt.ylabel(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a153a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load California Housing dataset\n",
    "df = pd.read_csv('/home/srujan-panchajanya-s-s/ML/california_housing_data.csv')\n",
    "# Table of Meaning of Each Variable\n",
    "variable_meaning = {\n",
    "    \"MedInc\": \"Median income in block group\",\n",
    "    \"HouseAge\": \"Median house age in block group\",\n",
    "    \"AveRooms\": \"Average number of rooms per household\",\n",
    "    \"AveBedrms\": \"Average number of bedrooms per household\",\n",
    "    \"Population\": \"Population of block group\",\n",
    "    \"AveOccup\": \"Average number of household members\",\n",
    "    \"Latitude\": \"Latitude of block group\",\n",
    "    \"Longitude\": \"Longitude of block group\",\n",
    "    \"Target\": \"Median house value (in $100,000s)\"\n",
    "}\n",
    "\n",
    "variable_df = pd.DataFrame(list(variable_meaning.items()), columns=[\"Feature\", \"Description\"])\n",
    "print(\"\\nVariable Meaning Table:\")\n",
    "print(variable_df)\n",
    "\n",
    "# Basic Data Exploration\n",
    "print(\"\\nBasic Information about Dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst Five Rows of Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Explanation of Summary Statistics\n",
    "summary_explanation = \"\"\"\n",
    "The summary statistics table provides key percentiles and other descriptive metrics.\n",
    "\n",
    "**25% (First Quartile - Q1):** This represents the value below which 25% of the data falls.\n",
    "**50% (Median - Q2):** This is the middle value when the data is sorted. It provides the central tendency.\n",
    "**75% (Third Quartile - Q3):** This represents the value below which 75% of the data falls.\n",
    "\n",
    "These percentiles are useful for detecting skewness, data distribution, and identifying potential outliers \n",
    "(values beyond Q1 - 1.5*IQR or Q3 + 1.5*IQR).\n",
    "\"\"\"\n",
    "print(\"\\nSummary Statistics Explanation:\")\n",
    "print(summary_explanation)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Histograms for distribution of features\n",
    "plt.figure(figsize=(12, 8))\n",
    "df.hist(figsize=(12, 8), bins=30, edgecolor='black')\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Boxplots for outlier detection\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplots of Features to Identify Outliers\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to analyze feature relationships (only a subset for clarity)\n",
    "sns.pairplot(df[['MedInc', 'HouseAge', 'AveRooms']], diag_kind='kde')\n",
    "plt.show()\n",
    "\n",
    "# Insights from Data Exploration\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"1. The dataset has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "print(\"2. No missing values were found in the dataset.\")\n",
    "print(\"3. Histograms show skewed distributions in some features like 'MedInc'.\")\n",
    "print(\"4. Boxplots indicate potential outliers in 'AveRooms' and 'AveOccup'.\")\n",
    "print(\"5. Correlation heatmap shows 'MedInc' has the highest correlation with house prices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5aaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = pd.read_csv(r'/home/srujan-panchajanya-s-s/ML/iris data set.csv')\n",
    "X = iris.iloc[:, :-1].values  # All columns except the last (features)\n",
    "y = iris.iloc[:, -1].values   # Last column (target/species)\n",
    "\n",
    "# Create numeric labels for species if they are strings\n",
    "if y.dtype == 'object':\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    target_names = le.classes_\n",
    "else:\n",
    "    target_names = ['setosa', 'versicolor', 'virginica']  # Default iris species names\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print(\"Eigenvalues:\\n\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = target_names\n",
    "\n",
    "for i in range(len(colors)):\n",
    "    ax.scatter(X_scaled[y == i, 0], X_scaled[y == i, 1], X_scaled[y == i, 2], color=colors[i], label=labels[i])\n",
    "\n",
    "ax.set_xlabel('Sepal Length')\n",
    "ax.set_ylabel('Sepal Width')\n",
    "ax.set_zlabel('Petal Length')\n",
    "ax.set_title('3D Visualization of Iris Data Before PCA')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "U, S, Vt = np.linalg.svd(X_scaled, full_matrices=False)\n",
    "print(\"Singular Values:\", S)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance by PC1: {explained_variance[0]:.2f}\")\n",
    "print(f\"Explained Variance by PC2: {explained_variance[1]:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(colors)):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=colors[i], label=labels[i])\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA on Iris Dataset (Dimensionality Reduction)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(len(colors)):\n",
    "    ax.scatter(X_scaled[y == i, 0], X_scaled[y == i, 1], X_scaled[y == i, 2], color=colors[i], label=labels[i])\n",
    "\n",
    "for i in range(3):  # Plot first three eigenvectors\n",
    "    ax.quiver(0, 0, 0, eigenvectors[i, 0], eigenvectors[i, 1], eigenvectors[i, 2], color='black', length=1)\n",
    "\n",
    "ax.set_xlabel('Sepal Length')\n",
    "ax.set_ylabel('Sepal Width')\n",
    "ax.set_zlabel('Petal Length')\n",
    "ax.set_title('3D Data with Eigenvectors')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bdbd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/srujan-panchajanya-s-s/ML/salgodataset.csv\")  # Replace with your CSV file path\n",
    "print(data)\n",
    "print(\"Training Data:\")\n",
    "def find_s_algorithm(data):\n",
    "    \"\"\"Implements the Find-S algorithm to find the most specific hypothesis.\"\"\"\n",
    "    attributes = data.iloc[:, :-1].values  # All columns except last\n",
    "    target = data.iloc[:, -1].values       # Last column (class labels)\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == \"Yes\":  # Consider only positive examples\n",
    "            hypothesis = attributes[i].copy()\n",
    "            break\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == \"Yes\":\n",
    "            for j in range(len(hypothesis)):\n",
    "                if hypothesis[j] != attributes[i][j]:\n",
    "                    hypothesis[j] = '?'  # Generalize inconsistent attributes\n",
    "    return hypothesis\n",
    "final_hypothesis = find_s_algorithm(data)\n",
    "print(\"Most Specific Hypothesis:\", final_hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "x_values = np.random.rand(100)\n",
    "labels = np.array([\"Class1\" if x <= 0.5 else \"Class2\" for x in x_values[:50]])\n",
    "print(x_values)\n",
    "print(\"--------------------------------------\")\n",
    "print(labels)\n",
    "def knn_classify(x_train, y_train, x_test, k):\n",
    "    predictions = []\n",
    "    for x in x_test:\n",
    "        distances = np.abs(x_train - x)  # Absolute distance\n",
    "        k_nearest_indices = np.argsort(distances)[:k]  # Indices of k nearest neighbors\n",
    "        k_nearest_labels = y_train[k_nearest_indices]  # Labels of nearest neighbors\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)[0][0]  # Most frequent label\n",
    "        predictions.append(most_common)\n",
    "    return np.array(predictions)\n",
    "k_values = [1, 2, 3, 4, 5, 20, 30]\n",
    "results = {}\n",
    "for k in k_values:\n",
    "    predicted_labels = knn_classify(x_values[:50], labels, x_values[50:], k)\n",
    "    results[k] = predicted_labels\n",
    "for k in k_values:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_values[:50], [1]*50, c=[\"blue\" if lbl == \"Class1\" else \"red\" for lbl in labels], label=\"Labeled Data\")\n",
    "    plt.scatter(x_values[50:], [2]*50, c=[\"blue\" if lbl == \"Class1\" else \"red\" for lbl in results[k]], label=f\"Classified Data (k={k})\")\n",
    "    plt.xlabel(\"x values\")\n",
    "    plt.ylabel(\"Classified/Unclassified\")\n",
    "    plt.title(f\"KNN Classification Clusters (k={k})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "for k, preds in results.items():\n",
    "    print(f\"Results for k={k}:\")\n",
    "    print(preds)\n",
    "    print(\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def gaussian_kernel(x, xi, tau):\n",
    "    return np.exp(-np.sum((x - xi) ** 2) / (2 * tau ** 2))\n",
    "def locally_weighted_regression(x, X, y, tau):\n",
    "    m = X.shape[0]\n",
    "    weights = np.array([gaussian_kernel(x, X[i], tau) for i in range(m)])\n",
    "    W = np.diag(weights)\n",
    "    X_transpose_W = X.T @ W\n",
    "    theta = np.linalg.inv(X_transpose_W @ X) @ X_transpose_W @ y\n",
    "    return x @ theta\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 2 * np.pi, 100)\n",
    "y = np.sin(X) + 0.1 * np.random.randn(100)\n",
    "X_bias = np.c_[np.ones(X.shape), X]\n",
    "x_test = np.linspace(0, 2 * np.pi, 200)\n",
    "x_test_bias = np.c_[np.ones(x_test.shape), x_test]\n",
    "tau = 0.5\n",
    "y_pred = np.array([locally_weighted_regression(xi, X_bias, y, tau) for xi in x_test_bias])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='red', label='Training Data', alpha=0.7)\n",
    "plt.plot(x_test, y_pred, color='blue', label=f'LWR Fit (tau={tau})', linewidth=2)\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Locally Weighted Regression', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# if want linear regresstion \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def gaussian_kernel(x, xi, tau):\n",
    "    return np.exp(-np.sum((x - xi) ** 2) / (2 * tau ** 2))\n",
    "def locally_weighted_regression(x, X, y, tau):\n",
    "    m = X.shape[0]\n",
    "    weights = np.array([gaussian_kernel(x, X[i], tau) for i in range(m)])\n",
    "    W = np.diag(weights)\n",
    "    X_transpose_W = X.T @ W\n",
    "    theta = np.linalg.inv(X_transpose_W @ X) @ X_transpose_W @ y\n",
    "    return x @ theta\n",
    "def linear_regression(X, y):\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return theta\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 2 * np.pi, 100)\n",
    "y = np.sin(X) + 0.1 * np.random.randn(100)\n",
    "X_bias = np.c_[np.ones(X.shape), X]\n",
    "x_test = np.linspace(0, 2 * np.pi, 200)\n",
    "x_test_bias = np.c_[np.ones(x_test.shape), x_test]\n",
    "tau = 0.5\n",
    "y_pred_lwr = np.array([locally_weighted_regression(xi, X_bias, y, tau) for xi in x_test_bias])\n",
    "theta = linear_regression(X_bias, y)\n",
    "y_pred_linear = x_test_bias @ theta\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='red', label='Training Data', alpha=0.7)\n",
    "plt.plot(x_test, y_pred_lwr, color='blue', label=f'LWR (tau={tau})', linewidth=2)\n",
    "plt.plot(x_test, y_pred_linear, color='green', label='Linear Regression', linewidth=2, linestyle='--')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Locally Weighted Regression vs Linear Regression')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "mse_lwr = np.mean((y - np.array([locally_weighted_regression(xi, X_bias, y, tau) for xi in X_bias])) ** 2)\n",
    "mse_linear = np.mean((y - X_bias @ theta) ** 2)\n",
    "print(f\"Linear Regression MSE: {mse_linear:.4f}\")\n",
    "print(f\"LWR MSE: {mse_lwr:.4f}\")\n",
    "print(f\"Linear parameters: intercept={theta[0]:.3f}, slope={theta[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c260056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(r\"/home/srujan-panchajanya-s-s/ML/dataset.csv\").dropna(subset=['horsepower', 'mpg'])\n",
    "X = df[['horsepower']]\n",
    "y = df['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "model = LinearRegression().fit(X_train_poly, y_train)\n",
    "X_range = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "X_range_poly = poly.transform(X_range)\n",
    "y_pred_range = model.predict(X_range_poly)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, color='blue', alpha=0.6, label='Data')\n",
    "plt.plot(X_range, y_pred_range, color='red', label='Polynomial Fit')\n",
    "plt.xlabel('Horsepower')\n",
    "plt.ylabel('MPG')\n",
    "plt.title('Polynomial Regression (Degree 2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#7.b\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "data = pd.read_csv(r\"/home/srujan-panchajanya-s-s/ML/Boston housing dataset.csv\")\n",
    "if data.isnull().values.any():\n",
    "    print(\"Filling missing values with column means.\")\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "X = data.drop(columns='MEDV')\n",
    "y = data['MEDV']\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.2f}, RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.6, edgecolors=\"k\", s=80)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.xlabel('Actual MEDV')\n",
    "plt.ylabel('Predicted MEDV')\n",
    "plt.title('Actual vs Predicted House Prices')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "print(\"\\n--- Decision Tree ---\")\n",
    "df = pd.read_csv(r'/home/srujan-panchajanya-s-s/ML/breastcancer.csv')\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, filled=True)\n",
    "plt.title('Decision Tree - Breast Cancer')\n",
    "plt.show()\n",
    "for i in y_pred:\n",
    "    if y_pred[i] == 0:\n",
    "        print(\"Prediction: Benign\")\n",
    "    else:\n",
    "        print(\"Prediction: Malignant\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92350416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "X, y = data.images, data.target\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Naive Bayes Classifier Accuracy: {accuracy * 100:.2f}%\")\n",
    "fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_test[i].reshape(64, 64), cmap='gray')\n",
    "    ax.set_title(f\"Pred: {y_pred[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "data = pd.read_csv(r'/home/srujan-panchajanya-s-s/ML/breastcancer.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X_scaled)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_kmeans))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_kmeans))\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "df['Cluster'] = y_kmeans\n",
    "df['True Label'] = y\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100,\n",
    "edgecolor='black', alpha=0.7)\n",
    "plt.title('K-Means Clustering of Breast Cancer Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='PC1', y='PC2', hue='True Label', palette='coolwarm',s=100, edgecolor='black', alpha=0.7)\n",
    "plt.title('True Labels of Breast Cancer Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title=\"True Label\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100,edgecolor='black', alpha=0.7)\n",
    "centers = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X',\n",
    "label='Centroids')\n",
    "plt.title('K-Means Clustering with Centroids')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.title('K-Means Clustering - Breast Cancer')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
